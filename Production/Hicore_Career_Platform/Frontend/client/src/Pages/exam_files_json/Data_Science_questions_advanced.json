[
  {
    "id": 5,
    "topic": "Descriptive Statistics",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is the median of the following set of numbers: 3, 7, 9, 5, 11?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "7"
      },
      {
        "label": "B",
        "type": "text",
        "value": "9"
      },
      {
        "label": "C",
        "type": "text",
        "value": "5"
      },
      {
        "label": "D",
        "type": "text",
        "value": "11"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "After arranging the numbers in ascending order (3, 5, 7, 9, 11), the median is the middle value, which is 7."
      }
    ]
  },
  {
    "id": 6,
    "topic": "Introduction to Data Science",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What does reproducible research in data science imply?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Research that can be duplicated by others using the same data and methods"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Research that has been peer-reviewed"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Research that results in the same findings regardless of the data used"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Research that can be published in scientific journals"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Reproducible research implies that the research can be duplicated by others using the same data and methods, ensuring the findings are reliable and verifiable."
      }
    ]
  },
  {
    "id": 4,
    "topic": "Basic Concepts of Machine Learning",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is the term used to describe the dataset used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Training Set"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Testing Set"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Validation Set"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Data Set"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "The validation set is used to evaluate a model's performance during the tuning of hyperparameters, providing an unbiased evaluation."
      }
    ]
  },
  {
    "id": 2,
    "topic": "Fundamental Concepts",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "In machine learning, what is the term used to describe the dataset used to fine-tune the model parameters?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Validation Set"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Training Set"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Test Set"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Inference Set"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "The validation set is used to evaluate a model, but it is not part of the training set. It helps in tuning the parameters of the model."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Basic Concepts of Data Visualization",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is the primary purpose of data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "To confuse the audience with complex representations"
      },
      {
        "label": "B",
        "type": "text",
        "value": "To communicate information clearly and efficiently"
      },
      {
        "label": "C",
        "type": "text",
        "value": "To display data without interpretation"
      },
      {
        "label": "D",
        "type": "text",
        "value": "To increase data processing time"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Data visualization aims to communicate information clearly and efficiently, making complex data more accessible and understandable."
      }
    ]
  },
  {
    "id": 2,
    "topic": "Introduction to Data Science",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "Who is recognized for coining the term 'Data Science'?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "William S. Cleveland"
      },
      {
        "label": "B",
        "type": "text",
        "value": "D.J. Patil"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Jeff Hammerbacher"
      },
      {
        "label": "D",
        "type": "text",
        "value": "John Tukey"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "William S. Cleveland is credited with coining the term 'Data Science' and advocating for an expansion of statistics beyond theory into technical computing."
      }
    ]
  },
  {
    "id": 9,
    "topic": "Basic Concepts",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is the primary purpose of data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "To confuse the audience with complex representations"
      },
      {
        "label": "B",
        "type": "text",
        "value": "To communicate information clearly and efficiently"
      },
      {
        "label": "C",
        "type": "text",
        "value": "To display data without interpretation"
      },
      {
        "label": "D",
        "type": "text",
        "value": "To increase the data processing time"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Data visualization aims to communicate information clearly and efficiently through graphical representations, making complex data more accessible and understandable."
      }
    ]
  },
  {
    "id": 8,
    "topic": "Model Evaluation",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is a common method for evaluating the performance of a machine learning model?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Random sampling"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Cross-validation"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Extrapolation"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Introspection"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Cross-validation is a robust method for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Descriptive Statistics",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is the most appropriate measure of central tendency for a data set with outliers?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Mean"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Median"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Mode"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Range"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "The median is the best measure of central tendency when a data set contains outliers because it is not as affected by extreme values as the mean."
      }
    ]
  },
  {
    "id": 1,
    "topic": "Basic Concepts of Data Visualization",
    "difficulty": "easy",
    "question": [
      {
        "type": "text",
        "value": "What is the primary purpose of data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "To confuse the audience with complex data."
      },
      {
        "label": "B",
        "type": "text",
        "value": "To create aesthetically pleasing abstract art."
      },
      {
        "label": "C",
        "type": "text",
        "value": "To communicate information clearly and efficiently."
      },
      {
        "label": "D",
        "type": "text",
        "value": "To encrypt sensitive information."
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Data visualization aims to communicate information clearly and efficiently via graphical representations, making complex data more accessible."
      }
    ]
  },
  {
    "id": 4,
    "topic": "Foundational Concepts of Data Science",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which of the following best describes the term 'reproducible research' in the context of data science?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Research that can be duplicated by others using the same data and methods"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Research that leads to the same conclusions, regardless of the data used"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Research that has been peer-reviewed and published"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Research that utilizes large datasets"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Reproducible research refers to research that can be exactly duplicated by other researchers using the same data and computational methods."
      }
    ]
  },
  {
    "id": 2,
    "topic": "Machine Learning Model Evaluation",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which of the following methods is used to evaluate the performance of a machine learning model by splitting the dataset into k consecutive folds?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Cross-validation"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Bootstrap aggregating"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Random subsampling"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Sequential partitioning"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Cross-validation is a model evaluation method that involves partitioning the data into subsets, training the model on subsets, and evaluating it on the complementary subset used as test data."
      }
    ]
  },
  {
    "id": 1,
    "topic": "Data Manipulation with Pandas",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which method in Pandas is used primarily for deleting any rows with missing data in a DataFrame?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "fillna()"
      },
      {
        "label": "B",
        "type": "text",
        "value": "dropna()"
      },
      {
        "label": "C",
        "type": "text",
        "value": "isnull()"
      },
      {
        "label": "D",
        "type": "text",
        "value": "notna()"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "The dropna() method in Pandas is used to remove missing values (NaN). fillna() fills the missing values, isnull() checks for missing values, and notna() checks for non-missing values."
      }
    ]
  },
  {
    "id": 4,
    "topic": "Data Distribution Visualization",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "What type of data visualization is used to represent the distribution of a dataset and identify outliers?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Histogram"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Box plot"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Heat map"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Word cloud"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Box plots are ideal for representing the distribution of data and identifying outliers through their quartiles and whiskers."
      }
    ]
  },
  {
    "id": 8,
    "topic": "Real-Time Data Handling",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which technology is specifically designed to handle real-time data processing and analysis?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Apache Cassandra"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Apache Hadoop"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Apache Spark"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Oracle Database"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Apache Spark is renowned for its ability to process real-time streaming data, making it ideal for real-time data handling and analysis."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Data Manipulation with Pandas",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which Pandas function is used to read a CSV file into a DataFrame?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "pandas.read_csv()"
      },
      {
        "label": "B",
        "type": "text",
        "value": "pandas.load_csv()"
      },
      {
        "label": "C",
        "type": "text",
        "value": "pandas.import_csv()"
      },
      {
        "label": "D",
        "type": "text",
        "value": "pandas.get_csv()"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "The function read_csv() is used in Pandas to read a comma-separated values (CSV) file into a Pandas DataFrame."
      }
    ]
  },
  {
    "id": 9,
    "topic": "Comparison of Visualization Techniques",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which graphical representation technique is primarily based on area instead of the height of bars?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Histogram"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Bar Chart"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Line Chart"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Pie Chart"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Histograms are based on the area of the bars to represent data frequencies over intervals, unlike bar charts which are based on the height of the bars."
      }
    ]
  },
  {
    "id": 6,
    "topic": "Confidence Intervals",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "A 95% confidence interval for a population mean is calculated as (18, 22). What is the correct interpretation of this interval?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "There is a 95% probability that the population mean is between 18 and 22."
      },
      {
        "label": "B",
        "type": "text",
        "value": "95% of the sample data falls between 18 and 22."
      },
      {
        "label": "C",
        "type": "text",
        "value": "If the experiment were repeated many times, 95% of the confidence intervals calculated from those experiments would include the true population mean."
      },
      {
        "label": "D",
        "type": "text",
        "value": "The sample mean is 20 with a margin of error of 2."
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "The correct interpretation of a 95% confidence interval is that if the same population is sampled under the same conditions multiple times, 95% of the calculated intervals would include the true population mean."
      }
    ]
  },
  {
    "id": 5,
    "topic": "Data Manipulation Libraries",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "Which of the following is NOT a Python library typically used for data manipulation?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "NumPy"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Pandas"
      },
      {
        "label": "C",
        "type": "text",
        "value": "TensorFlow"
      },
      {
        "label": "D",
        "type": "text",
        "value": "SciPy"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "TensorFlow is primarily used for machine learning and neural networks, not for general data manipulation tasks like NumPy, Pandas, and SciPy."
      }
    ]
  },
  {
    "id": 8,
    "topic": "Hypothesis Testing",
    "difficulty": "medium",
    "question": [
      {
        "type": "text",
        "value": "In a hypothesis testing, if the p-value is less than the significance level (alpha), what is the appropriate action?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Accept the null hypothesis."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Reject the null hypothesis."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Increase the sample size."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Conduct a one-tailed test."
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "If the p-value is less than the significance level, we have sufficient evidence to reject the null hypothesis, thus making option B correct."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Data Handling in Big Data",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "In the context of real-time data processing, which big data technology would be most appropriate for processing streams of data with complex event processing capabilities?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Hadoop"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Apache Kafka"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Apache Storm"
      },
      {
        "label": "D",
        "type": "text",
        "value": "MongoDB"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Apache Storm is specifically designed for real-time processing and is capable of handling complex event streams efficiently, making it the most suitable for the described requirement."
      }
    ]
  },
  {
    "id": 1,
    "topic": "Histograms vs Bar Charts",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "In data visualization, what is the fundamental difference between histograms and bar charts?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Histograms are used for categorical data whereas bar charts are used for numerical data distributions."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Bar charts are used for categorical data whereas histograms are used for numerical data distributions."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Histograms must use logarithmic scales, but bar charts do not."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Bar charts represent only discrete data, histograms represent only continuous data."
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Histograms represent the distribution of numerical data by showing the frequencies of data points within certain ranges of values (bins), ideal for continuous data. Bar charts, on the other hand, are used to compare different categories of data, making them suitable for categorical data."
      }
    ]
  },
  {
    "id": 9,
    "topic": "Comparative Analysis of Visualization Techniques",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following statements correctly differentiates between a histogram and a bar chart in the context of data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Histograms are used for continuous data while bar charts are used for categorical data."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Histograms and bar charts are both used primarily for categorical data representation."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Bar charts are based on area, whereas histograms are based on height."
      },
      {
        "label": "D",
        "type": "text",
        "value": "There is no significant difference; both visualize data through vertical bars."
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Histograms represent the distribution of numerical data by indicating the frequency of data points within certain ranges (bins), suitable for continuous data. Bar charts, on the other hand, compare different categories with heights of bars, ideal for categorical data."
      }
    ]
  },
  {
    "id": 2,
    "topic": "Evolution of Data Science",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Considering the historical development and contemporary applications, which of the following statements accurately reflects the impact of data science on modern industries?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Data science has had minimal impact on the healthcare industry."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Data science is primarily used in the technology sector, with little application elsewhere."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Data science has revolutionized decision-making processes across various sectors, including healthcare, finance, and retail."
      },
      {
        "label": "D",
        "type": "text",
        "value": "The applications of data science are limited to machine learning and artificial intelligence."
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Data science has broadly impacted multiple sectors by enhancing decision-making processes with data-driven insights, significantly evident in industries like healthcare, finance, and retail."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Loss Functions",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following statements is accurate regarding the use of Mean Absolute Error (MAE) versus Mean Squared Error (MSE) in training regression models?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "MAE is more robust to outliers than MSE."
      },
      {
        "label": "B",
        "type": "text",
        "value": "MSE generally converges slower than MAE due to the squaring of errors."
      },
      {
        "label": "C",
        "type": "text",
        "value": "MAE leads to more complex gradients which complicate the optimization process."
      },
      {
        "label": "D",
        "type": "text",
        "value": "MSE is less sensitive to variations in the data compared to MAE."
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "MAE is more robust to outliers because it does not square the error terms, which can disproportionately affect the model's loss evaluation when outliers are present, unlike MSE where the squaring of larger error terms can lead to an overemphasis on outliers."
      }
    ]
  },
  {
    "id": 6,
    "topic": "Big Data Storage Technologies",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which technology is pivotal for creating Data Lakes, supporting both structured and unstructured data?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "MongoDB"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Cassandra"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Apache Hadoop"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Redis"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Apache Hadoop is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is particularly useful in building data lakes that handle both structured and unstructured data."
      }
    ]
  },
  {
    "id": 5,
    "topic": "Advanced Data Manipulation",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Considering a DataFrame 'df' with a DateTime index and a column 'A', which line of code will resample the data to provide the mean value of 'A' for the beginning of each month?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "df.resample('M').mean()"
      },
      {
        "label": "B",
        "type": "text",
        "value": "df.resample('MS').mean()"
      },
      {
        "label": "C",
        "type": "text",
        "value": "df.groupby('M').mean()"
      },
      {
        "label": "D",
        "type": "text",
        "value": "df.groupby('MS').mean()"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "The 'MS' string is the correct frequency string to resample the data at the start ('S') of each month ('M'). The 'mean()' function then computes the mean of 'A' for each resulting period."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Advanced Techniques in Data Visualization",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following is NOT a recommended practice when creating heat maps for data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Using a wide range of contrasting colors to represent different data ranges."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Applying a logarithmic scale when the data range is extremely large."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Utilizing similar colors for adjacent data values to ensure readability."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Including detailed textual descriptions inside each heat map cell."
      }
    ],
    "correct": "D",
    "explanation": [
      {
        "type": "text",
        "value": "Including detailed textual descriptions inside each heat map cell is not recommended as it can clutter the visualization and reduce clarity. Heat maps should be visually intuitive and typically use color gradients to denote data variations."
      }
    ]
  },
  {
    "id": 9,
    "topic": "Data Science Evolution",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following best describes the shift in data science practice from 1980s to the present day?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Shift from primarily statistical models to a blend of statistics and machine learning."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Transition from manual data recording to automated data capture systems."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Move from using predominantly proprietary data to exclusively open-source data."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Change from individual data analysis to collaborative cloud-based analysis."
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "The correct answer is A. Over the years, data science has evolved from focusing mainly on statistical models to incorporating machine learning techniques, allowing for more complex and predictive analytics."
      }
    ]
  },
  {
    "id": 2,
    "topic": "Loss Functions",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which loss function would be most appropriate for a regression model that is sensitive to outliers in the dataset?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Mean Squared Error (MSE)"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Mean Absolute Error (MAE)"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Cross-Entropy Loss"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Huber Loss"
      }
    ],
    "correct": "D",
    "explanation": [
      {
        "type": "text",
        "value": "Huber Loss is more robust to outliers than the commonly used Mean Squared Error. It combines the best properties of MSE and Mean Absolute Error (MAE), behaving like MSE for smaller errors and MAE for large errors, which helps in reducing the influence of outliers."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Evolution of Data Science",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following best describes the historical development of data science as a formal discipline?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Data science originated solely from statistics with minimal contributions from computer science."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Data science is a new field that emerged in the 21st century without any historical precedents."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Data science evolved through the convergence of statistics, computer science, and domain-specific knowledge."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Data science has remained unchanged since its inception and has not been influenced by technological advancements."
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Data science has grown as an interdisciplinary field, incorporating elements of statistics, computer science, and specific knowledge from various domains, which reflects its comprehensive and evolving nature."
      }
    ]
  },
  {
    "id": 2,
    "topic": "Binomial Distribution",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "A biased coin is flipped 10 times. The probability of getting a head in a single flip is 0.2. What is the probability of getting exactly 2 heads in 10 flips?"
      },
      {
        "type": "formula",
        "value": "P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "formula",
        "value": "0.3020"
      },
      {
        "label": "B",
        "type": "formula",
        "value": "0.3758"
      },
      {
        "label": "C",
        "type": "formula",
        "value": "0.2013"
      },
      {
        "label": "D",
        "type": "formula",
        "value": "0.1209"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Using the binomial formula P(X = 2) = \\binom{10}{2} (0.2)^2 (0.8)^8 = 45 * 0.04 * 0.16777216 = 0.3020. Hence, the probability of getting exactly 2 heads in 10 flips is 0.3020."
      }
    ]
  },
  {
    "id": 1,
    "topic": "Reproducible Research",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following best describes the components necessary for conducting reproducible research in Data Science?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Source code, random seed, and proprietary data"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Source code, version control systems, and open-source datasets"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Source code only"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Data visualization tools, source code, and random seed"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Reproducible research in Data Science requires the sharing of source code, use of version control systems, and open-source datasets to ensure others can replicate the results."
      }
    ]
  },
  {
    "id": 6,
    "topic": "Reproducible Research",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "What does the term 'reproducible research' imply in the context of data science?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Research that can be duplicated with different tools"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Research that leads to the same conclusions when analyzed by different teams"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Research that can be exactly duplicated by others using the same data and methods"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Research that is published in multiple journals"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Reproducible research refers to research that can be exactly duplicated by other researchers using the same data and methods, ensuring the validity and reliability of the findings."
      }
    ]
  },
  {
    "id": 5,
    "topic": "Histograms vs. Bar Charts",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following is a key difference between histograms and bar charts in data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Histograms display categorical data, whereas bar charts display numerical data distributions."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Histograms are used to show distributions of variables, whereas bar charts compare variables."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Bar charts use vertical lines at each data point, unlike histograms."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Bar charts require a zero-baseline, but histograms do not."
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Histograms are specifically used for showing the frequency distribution of quantitative data, whereas bar charts are used to compare different categories of data."
      }
    ]
  },
  {
    "id": 7,
    "topic": "Real-Time Big Data Processing",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "What is a significant challenge when implementing real-time data processing in big data technologies?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Ensuring data privacy and security"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Reducing the cost of data storage"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Choosing the right data visualization tools"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Simplifying the user interface"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "While all options present valid concerns, ensuring data privacy and security is particularly challenging in real-time processing due to the continuous flow and accessibility of data."
      }
    ]
  },
  {
    "id": 8,
    "topic": "Hybrid Cloud and Big Data",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following best describes the role of hybrid cloud technologies in big data storage?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Hybrid cloud technologies are primarily used for data deletion."
      },
      {
        "label": "B",
        "type": "text",
        "value": "They facilitate dynamic resource allocation between on-premises and cloud environments."
      },
      {
        "label": "C",
        "type": "text",
        "value": "They are used exclusively for unstructured data."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Hybrid clouds are less secure than traditional data storage methods."
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Hybrid cloud technologies enable dynamic resource allocation which helps in managing big data by leveraging both on-premises and cloud resources effectively, thus providing flexibility and scalability."
      }
    ]
  },
  {
    "id": 10,
    "topic": "Binomial Distribution",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "A biased coin with a probability of heads being 0.65 is flipped 10 times. What is the probability of getting exactly 7 heads?"
      },
      {
        "type": "formula",
        "value": "P(X = 7) = \\binom{10}{7} \\times (0.65)^7 \\times (0.35)^3"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "formula",
        "value": "0.278"
      },
      {
        "label": "B",
        "type": "formula",
        "value": "0.165"
      },
      {
        "label": "C",
        "type": "formula",
        "value": "0.350"
      },
      {
        "label": "D",
        "type": "formula",
        "value": "0.430"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Use the binomial probability formula with n = 10, k = 7, p = 0.65 to calculate the probability. This gives us P(X = 7) = \\binom{10}{7} \\times (0.65)^7 \\times (0.35)^3 = 0.278."
      }
    ]
  },
  {
    "id": 6,
    "topic": "Bias-Variance Tradeoff",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "In the context of the bias-variance tradeoff, what is generally true as the complexity of a machine learning model increases?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Variance decreases, bias increases"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Variance increases, bias decreases"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Both variance and bias increase"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Both variance and bias decrease"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "As the complexity of a model increases, it fits the training data more closely, leading to a decrease in bias but an increase in variance as the model begins to overfit the data."
      }
    ]
  },
  {
    "id": 9,
    "topic": "Big Data Storage Technologies",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following data storage solutions is least likely to be used for handling large-scale, unstructured big data?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "NoSQL databases"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Data Lakes"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Relational Databases"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Hadoop Distributed File System (HDFS)"
      }
    ],
    "correct": "C",
    "explanation": [
      {
        "type": "text",
        "value": "Relational Databases are traditionally used for structured data and are less efficient for handling large-scale, unstructured big data compared to NoSQL databases, Data Lakes, or HDFS."
      }
    ]
  },
  {
    "id": 7,
    "topic": "Bias-Variance Tradeoff",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Consider a machine learning model with high variance and low bias. What technique would be most effective in addressing the issue of high variance?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Increasing the regularization parameter"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Adding more features to the model"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Using a simpler model with fewer parameters"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Collecting more training data"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Increasing the regularization parameter helps to reduce model complexity, thus addressing overfitting and effectively reducing variance without significantly increasing bias."
      }
    ]
  },
  {
    "id": 7,
    "topic": "Comparative Analysis of Visualization Techniques",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following statements correctly differentiates between histograms and bar charts in the context of data visualization?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Histograms are used for qualitative data, while bar charts are more suitable for quantitative data."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Bar charts represent data using rectangular bars with lengths proportional to the values they represent, whereas histograms use bars to display the frequency of data intervals."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Histograms and bar charts are essentially the same, as both are used to compare different categories of data."
      },
      {
        "label": "D",
        "type": "text",
        "value": "Bar charts are based on area, not the height of bars, while histograms use height to represent frequencies."
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "Bar charts and histograms may look similar but serve different purposes. Bar charts are used to compare different categories, where each bar represents a category and its height or length corresponds to its value. Histograms, on the other hand, are used to show distributions of data and are particularly useful for showing the frequency of data points within certain ranges, making them ideal for quantitative data."
      }
    ]
  },
  {
    "id": 6,
    "topic": "Complex Data Transformation",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Given a multi-dimensional NumPy array 'arr', which function call will correctly apply a logarithmic transformation to every element in the array?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "np.log(arr)"
      },
      {
        "label": "B",
        "type": "text",
        "value": "np.apply_along_axis(np.log, 1, arr)"
      },
      {
        "label": "C",
        "type": "text",
        "value": "np.log10(arr)"
      },
      {
        "label": "D",
        "type": "text",
        "value": "arr.apply(np.log)"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "The function 'np.log(arr)' directly applies the natural logarithm to each element of the array 'arr'. The other options either do not apply the correct function, apply a different logarithmic base, or use incorrect syntax."
      }
    ]
  },
  {
    "id": 7,
    "topic": "Statistical Inference",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Given a sample size of 30 from a normally distributed population, you want to construct a 95% confidence interval for the population mean. The sample mean is 200, and the sample standard deviation is 50. Which of the following is the correct confidence interval?"
      },
      {
        "type": "formula",
        "value": "CI = \\bar{x} \\pm z^{*} \\left(\\frac{s}{\\sqrt{n}}\\right)"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "formula",
        "value": "[185.6, 214.4]"
      },
      {
        "label": "B",
        "type": "formula",
        "value": "[190.1, 209.9]"
      },
      {
        "label": "C",
        "type": "formula",
        "value": "[180.2, 219.8]"
      },
      {
        "label": "D",
        "type": "formula",
        "value": "[175.3, 224.7]"
      }
    ],
    "correct": "B",
    "explanation": [
      {
        "type": "text",
        "value": "To construct the confidence interval, use the formula for the confidence interval of the mean with known standard deviation. Using z*=1.96 for 95% CI and substituting the given values, we calculate (200 \u00b1 1.96\u00d7(50/\u221a30)). This calculation gives the interval [190.1, 209.9]."
      }
    ]
  },
  {
    "id": 3,
    "topic": "Conditional Probability",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Given two events A and B in a probability space, where P(A) = 0.5, P(B) = 0.4, and P(A \u2229 B) = 0.2, what is P(A|B)?"
      },
      {
        "type": "formula",
        "value": "P(A|B) = \\frac{P(A \\cap B)}{P(B)}"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "formula",
        "value": "0.5"
      },
      {
        "label": "B",
        "type": "formula",
        "value": "0.4"
      },
      {
        "label": "C",
        "type": "formula",
        "value": "0.2"
      },
      {
        "label": "D",
        "type": "formula",
        "value": "0.5"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Using the formula for conditional probability, P(A|B) = P(A \u2229 B) / P(B) = 0.2 / 0.4 = 0.5."
      }
    ]
  },
  {
    "id": 5,
    "topic": "Conditional Probability",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Given two events A and B where P(A) = 0.5, P(B) = 0.3, and P(A \u2229 B) = 0.2, what is P(A | B)?"
      },
      {
        "type": "formula",
        "value": "P(A | B) = \\frac{P(A \\cap B)}{P(B)}"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "formula",
        "value": "0.67"
      },
      {
        "label": "B",
        "type": "formula",
        "value": "0.15"
      },
      {
        "label": "C",
        "type": "formula",
        "value": "0.8"
      },
      {
        "label": "D",
        "type": "formula",
        "value": "0.5"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "P(A | B) is calculated as the probability of A given B, which is the intersection of A and B divided by the probability of B. Using the formula P(A | B) = P(A \u2229 B) / P(B), we get P(A | B) = 0.2 / 0.3 = 0.67."
      }
    ]
  },
  {
    "id": 2,
    "topic": "NumPy and Data Analysis",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "In NumPy, how can you filter out all entries from a 2D array 'arr' which have a sum less than 10 across their rows?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "arr[arr.sum(axis=1) < 10]"
      },
      {
        "label": "B",
        "type": "text",
        "value": "arr.sum(axis=1, where=(arr < 10))"
      },
      {
        "label": "C",
        "type": "text",
        "value": "arr[:, arr.sum(axis=0) < 10]"
      },
      {
        "label": "D",
        "type": "text",
        "value": "arr.sum(axis=1) < 10"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "Using 'arr.sum(axis=1) < 10' generates a boolean array where each element corresponds to whether the sum of the elements in that row is less than 10. This boolean array is then used to index 'arr', filtering out the rows that meet this condition."
      }
    ]
  },
  {
    "id": 10,
    "topic": "Bias-Variance Tradeoff",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "Which of the following best illustrates the bias-variance tradeoff in machine learning model training?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "Increasing model complexity will usually decrease bias, potentially at the cost of increased variance."
      },
      {
        "label": "B",
        "type": "text",
        "value": "Decreasing the number of features always decreases the model variance."
      },
      {
        "label": "C",
        "type": "text",
        "value": "Using regularization techniques always increases the model's bias."
      },
      {
        "label": "D",
        "type": "text",
        "value": "A higher model bias is preferable for complex datasets."
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "The bias-variance tradeoff is a fundamental concept where increasing model complexity (e.g., more parameters) tends to decrease bias, as the model can better capture the data complexity. However, this usually increases variance as the model becomes too tailored to the training data, potentially leading to overfitting."
      }
    ]
  },
  {
    "id": 8,
    "topic": "Advanced Data Representation Techniques",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "In the context of creating interactive data visualizations, which JavaScript library would be most appropriate for real-time data updates in complex visual analytics?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "D3.js"
      },
      {
        "label": "B",
        "type": "text",
        "value": "Chart.js"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Highcharts"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Leaflet"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "D3.js (Data-Driven Documents) is highly suitable for complex and real-time data visualizations as it uses web standards like HTML, SVG, and CSS. Its powerful rendering capabilities allow for the efficient manipulation of documents based on data, making it ideal for applications that require frequent data updates and interactive visual components."
      }
    ]
  },
  {
    "id": 1,
    "topic": "Regularization Techniques",
    "difficulty": "hard",
    "question": [
      {
        "type": "text",
        "value": "In the context of preventing overfitting in machine learning models, which regularization technique can be described as adding the sum of the absolute values of coefficients to the loss function of the model?"
      }
    ],
    "options": [
      {
        "label": "A",
        "type": "text",
        "value": "L1 regularization"
      },
      {
        "label": "B",
        "type": "text",
        "value": "L2 regularization"
      },
      {
        "label": "C",
        "type": "text",
        "value": "Dropout"
      },
      {
        "label": "D",
        "type": "text",
        "value": "Early stopping"
      }
    ],
    "correct": "A",
    "explanation": [
      {
        "type": "text",
        "value": "L1 regularization, also known as Lasso regression, adds the sum of the absolute values of the coefficients as a penalty term to the loss function. This can lead to sparsity in the model coefficients as some coefficients can become zero."
      }
    ]
  }
]